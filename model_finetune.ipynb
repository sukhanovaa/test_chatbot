{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A general plan\n",
    "* Explore metrics used to evaluate conversational flow\n",
    "* Choose an open-source model that \n",
    "    1. fits my hardware capacity\n",
    "    2. has nice metrics\n",
    "* Choose conversational datasets to finetune\n",
    "* Prepare said datasets\n",
    "* Add LoRA code - thanks to yandexdataschool for additional guidance!\n",
    "* Finetune the model (using huggingface trainer)\n",
    "\n",
    "...then wrap the model into a container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore metrics\n",
    "Naive, regarding interaction statistics\n",
    "* Length of conversation: how long (in user lines) lasts a chat on average\n",
    "* Time between replies to the chatbot, which could indicate the level of a user's engagement. Requires analytics to establish thresholds.\n",
    "* Human feedback, as in\n",
    "    - \"Were you satisfied by the conversation? Y/N\" at the end of the conversation - does not give fine-grained information about what happened during the conversation that led to a particular assessment\n",
    "    - grade 1-10 - just as above. \n",
    "    - This is overall messy, but it makes sense for such feedback to be collected, then post-analyzed by assessors.\n",
    "* How often do users come back to chat again? Need users!\n",
    "* How often is a chat with a model abandoned at the start? Need users!\n",
    "\n",
    "\n",
    "Naive, based on text\n",
    "* repetition / fluency, as in N of distinct n-grams - the only quantifiable metric. However, it really is just proxy metrics for generation quality; does not measure the adequacy of the conversation...\n",
    "* perplexity on the user questions? - this is not about answers' quality at all; besides, I doubt that huge models have difficulties in this domain\n",
    "* overlap between a user's question and a model's answer - I think this is a bit outdated for LLM evaluation, since it used to be a problem around 2017-2020 maybe. Could make sense if we have enough time to check the LM's answer before outputting it to the user.\n",
    "* F-measure/BLEU/etc. on questions with pre-defined answers? - might be good for tracking factual information, but the good chit-chat has nothing pre-defined a priori. There are separate tasks that could be used to see the prompted model performance though, e.g. summarization.\n",
    "* Embedding distance between the user's question and the model's answer - seems pretty much the same as the previous idea\n",
    "* % of negative user responses (\"no, that's not what I was talking about\"; \"that's a dumb response\"; \"bad bot\"); % of positive user responses (\"You're funny!\", \"Great, thanks\", \"you're right\")\n",
    "    - very hard to define 'negative', but theoretically it could be (embedding) cosine similarity to sets of responses similar to above; results in a problem of constructing such sets, extracting user responses from paragraphs, etc.\n",
    "    - metrics such as these are really very much user-dependent (what if a user is often ironic? what if they're displaying opinions far from a bot's training data distribution? what if a user is in a bad mood...)\n",
    "    - but __could be captured__ if reframed as an entailment task (LM-user answer pairs, entailment/contradiction/neutral; we're looking for contradictions; something like https://arxiv.org/abs/1904.03371 ?) or as a sentiment analysis task, but requires additional models\n",
    "\n",
    "I actually expect that interaction-based metrics per model are much more telling than the textual ones.\n",
    "\n",
    "\n",
    "There are also specialized metrics involving various aspects of quality, e.g. empathy: https://github.com/Sea94/ieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a model\n",
    "\n",
    "A recent causal LM that fits my memory is all I ask, so under 13B\n",
    "* OPT? Comes in flavours like < 1B, 1.3B, 2.7B, 6.7B\n",
    "* GPT-J? 6B\n",
    "* LLaMA? 7B\n",
    "\n",
    "I looked at https://weightwatcher.ai/leaderboard.html and https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard and compared the top models regarding their Alpha and Average, respectively. Frankly, these two metrics seem to be contradicting each other. \n",
    "\n",
    "Surprisingly, an OPT-1.3B model was pretty small, high in Alpha and decent in terms of Truthfulness, although other metrics were not as impressive. Since I haven't had any opinion based on experience, I decided to test the pipeline on a small OPT. \n",
    "\n",
    "If I have enough time, I'll switch to the fresh LLaMA-2 (7B) -- it demonstrates a good average on the leaderboard tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers datasets peft accelerate bitsandbytes sacremoses pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-1.3b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose datasets & prepare them\n",
    "\n",
    "Personally, I think that all dialogue corpora ought to be curated. As a source of data, Reddit probably contains most lively responses, but it is potentially unpredictable and should be tested first. I considered the following datasets:\n",
    "\n",
    "* ConvAI2/3 dataset - on the closer look, it has a deeper structure that is useful in RLHF or evaluation contexts\n",
    "* The NPS Chat Corpus - not in Huggingface Datasets, but potentially useful\n",
    "* Cornell Movie-Dialogs Corpus\n",
    "* HC3 (which also contains ChatGPT answers, but we won't need them)\n",
    "* and DailyDialogue corpus\n",
    "\n",
    "\n",
    "And then the problems started.\n",
    "\n",
    "- I haven't opened a PR yet, but the builder script for 'cornell_movie_dialog' is broken\n",
    "    \n",
    "    \\# y = load_dataset('cornell_movie_dialog')['train']  \n",
    "- 'Hello-SimpleAI/HC3' is broken too\n",
    "    \n",
    "    \\# z = load_dataset('Hello-SimpleAI/HC3')['train'] # ['question', 'human_answers']\n",
    "- The NPS Chat Corpus is a part of NLTK, and it's completely unusable because it is a loose collection of posts and not (somewhat coherent) dialogs. \n",
    "\n",
    "\n",
    "I was left with DailyDialogue and manually-processed HC3.\n",
    "\n",
    "By the way, these should've worked better if the data also contained prompts (like 'USER1 says '), in a manner in which the chatbot is prompted (see src/chatbot.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "# by the way, daily_dialogs is suddenly pre-tokenized in some places\n",
    "from sacremoses import MosesDetokenizer\n",
    "\n",
    "dd = load_dataset('daily_dialog') # ['dialog']\n",
    "detok = MosesDetokenizer('en')\n",
    "\n",
    "def process_daily_dialogs(examples):\n",
    "    temp = [[detok.detokenize(x.split()) for x in y] for y in examples['dialog']]\n",
    "    temp = [' | '.join(x) for x in temp]\n",
    "    examples = tokenizer(temp)\n",
    "    return examples\n",
    "\n",
    "dd['train'] = dd['train'].map(process_daily_dialogs, batched=True, remove_columns=[\"dialog\", 'act', 'emotion'])\n",
    "dd['test'] = dd['test'].map(process_daily_dialogs, batched=True, remove_columns=[\"dialog\", 'act', 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92220ceb9c5a44568f56e71536dacb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21889 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea32e2802e9450eb69b6da6abedb10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2433 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !wget https://huggingface.co/datasets/Hello-SimpleAI/HC3/resolve/main/all.jsonl\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_hc3():\n",
    "    with open(\"all.jsonl\") as inp:\n",
    "        data = pd.DataFrame.from_records([json.loads(line) for line in inp])\n",
    "    return data\n",
    "\n",
    "hc3 = Dataset.from_pandas(load_hc3()) # 'question', 'human_answers'\n",
    "hc3 = hc3.train_test_split(test_size=0.1)\n",
    "\n",
    "def process_hc3(examples):\n",
    "    # This could have been done in pandas, I think...\n",
    "    pairs = []\n",
    "    for x, y in zip(examples['question'], examples['human_answers']):\n",
    "        x = [detok.detokenize(x.split())] * len(y)\n",
    "        y = [detok.detokenize(t.split()) for t in y]\n",
    "        pairs.extend([' | '.join(d) for d in zip(x, y)])\n",
    "    examples = tokenizer(pairs)\n",
    "    return examples\n",
    "    \n",
    "hc3['train'] = hc3['train'].map(process_hc3, batched=True, remove_columns=[\"question\", 'human_answers', 'chatgpt_answers', 'index', 'source'])\n",
    "hc3['test'] = hc3['test'].map(process_hc3, batched=True, remove_columns=[\"question\", 'human_answers', 'chatgpt_answers', 'index', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "data_train = concatenate_datasets([dd['train'], hc3['train']]) \n",
    "data_test = concatenate_datasets([dd['test'], hc3['test']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "I'll use one PEFT-LoRa example, because I don't really have a lot of time to look deeper than that. I suppose that Prefix Tuning could work better, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alenasuhanova/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
    "\n",
    "# I've seen people use int4 in BitsAndBytes for training; a bit too extreme for a baseline\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map={\"\":0})\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='cpu')  #load_in_8bit=True\n",
    "# model = prepare_model_for_int8_training(model)  # prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], \n",
    "                    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model = model.float()\n",
    "# model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "loader = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d117c5eb070748b7be1859de6eef28b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 18.1293, 'train_samples_per_second': 0.055, 'train_steps_per_second': 0.055, 'train_loss': 3.4671413898468018, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        auto_find_batch_size=False,\n",
    "        # warmup_steps=250,\n",
    "        # max_steps=5000,\n",
    "        warmup_steps=1,\n",
    "        max_steps=1,\n",
    "        learning_rate=2e-4,  # ?\n",
    "        no_cuda=True, \n",
    "        logging_steps=250,\n",
    "        output_dir=\"outputs\",\n",
    "        # optim=\"paged_adamw_8bit\"\n",
    "        optim='adamw_torch'\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_test,\n",
    "    args=args,\n",
    "    data_collator=loader\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained('weights')\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path,\n",
    "                                            # load_in_8bit=True, \n",
    "                                            device_map='cpu')\n",
    "model = PeftModel.from_pretrained(model, 'weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
