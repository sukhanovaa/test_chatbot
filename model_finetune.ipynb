{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A general plan\n",
    "* Explore metrics used to evaluate conversational flow\n",
    "* Choose an open-source model that \n",
    "    1. fits my hardware capacity\n",
    "    2. has nice metrics\n",
    "* Choose conversational datasets to finetune\n",
    "* Prepare said datasets\n",
    "* Add LoRA code - thanks to yandexdataschool for additional guidance!\n",
    "* Finetune the model (using huggingface trainer)\n",
    "\n",
    "...then wrap the model into a container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore metrics\n",
    "Naive, regarding interaction statistics\n",
    "* Length of conversation: how long (in user lines) lasts a chat on average\n",
    "* Time between replies to the chatbot, which could indicate the level of a user's engagement. Requires analytics to establish thresholds.\n",
    "* Human feedback, as in\n",
    "    - \"Were you satisfied by the conversation? Y/N\" at the end of the conversation - does not give fine-grained information about what happened during the conversation that led to a particular assessment\n",
    "    - grade 1-10 - just as above. \n",
    "    - This is overall messy, but it makes sense for such feedback to be collected, then post-analyzed by assessors.\n",
    "* How often do users come back to chat again? Need users!\n",
    "* How often is a chat with a model abandoned at the start? Need users!\n",
    "\n",
    "\n",
    "Naive, based on text\n",
    "* repetition / fluency, as in N of distinct n-grams - the only quantifiable metric. However, it really is just proxy metrics for generation quality; does not measure the adequacy of the conversation...\n",
    "* perplexity on the user questions? - this is not about answers' quality at all; besides, I doubt that huge models have difficulties in this domain\n",
    "* overlap between a user's question and a model's answer - I think this is a bit outdated for LLM evaluation, since it used to be a problem around 2017-2020 maybe. Could make sense if we have enough time to check the LM's answer before outputting it to the user.\n",
    "* F-measure/BLEU/etc. on questions with pre-defined answers? - might be good for tracking factual information, but the good chit-chat has nothing pre-defined a priori. There are separate tasks that could be used to see the prompted model performance though, e.g. summarization.\n",
    "* Embedding distance between the user's question and the model's answer - seems pretty much the same as the previous idea\n",
    "* % of negative user responses (\"no, that's not what I was talking about\"; \"that's a dumb response\"; \"bad bot\"); % of positive user responses (\"You're funny!\", \"Great, thanks\", \"you're right\")\n",
    "    - very hard to define 'negative', but theoretically it could be (embedding) cosine similarity to sets of responses similar to above; results in a problem of constructing such sets, extracting user responses from paragraphs, etc.\n",
    "    - metrics such as these are really very much user-dependent (what if a user is often ironic? what if they're displaying opinions far from a bot's training data distribution? what if a user is in a bad mood...)\n",
    "    - but __could be captured__ if reframed as an entailment task (LM-user answer pairs, entailment/contradiction/neutral; we're looking for contradictions; something like https://arxiv.org/abs/1904.03371 ?) or as a sentiment analysis task, but requires additional models\n",
    "\n",
    "I actually expect that interaction-based metrics per model are much more telling than the textual ones.\n",
    "\n",
    "\n",
    "There are also specialized metrics involving various aspects of quality, e.g. empathy: https://github.com/Sea94/ieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a model\n",
    "\n",
    "A recent causal LM that fits my memory is all I ask, so under 13B\n",
    "* OPT? Comes in flavours like < 1B, 1.3B, 2.7B, 6.7B\n",
    "* GPT-J? 6B\n",
    "* LLaMA? 7B\n",
    "\n",
    "I looked at https://weightwatcher.ai/leaderboard.html and https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard and compared the top models regarding their Alpha and Average, respectively. Frankly, these two metrics seem to be contradicting each other. \n",
    "\n",
    "Surprisingly, an OPT-1.3B model was pretty small, high in Alpha and decent in terms of Truthfulness, although other metrics were not as impressive. Since I haven't had any opinion based on experience, I decided to test the pipeline on a small OPT. \n",
    "\n",
    "If I have enough time, I'll switch to the fresh LLaMA-2 (7B) -- it demonstrates a good average on the leaderboard tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers datasets peft accelerate bitsandbytes sacremoses pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-1.3b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose datasets & prepare them\n",
    "\n",
    "Personally, I think that all dialogue corpora ought to be curated. As a source of data, Reddit probably contains most lively responses, but it is potentially unpredictable and should be tested first. I considered the following datasets:\n",
    "\n",
    "* ConvAI2/3 dataset - on the closer look, it has a deeper structure that is useful in RLHF or evaluation contexts\n",
    "* The NPS Chat Corpus - not in Huggingface Datasets, but potentially useful\n",
    "* Cornell Movie-Dialogs Corpus\n",
    "* HC3 (which also contains ChatGPT answers, but we won't need them)\n",
    "* and DailyDialogue corpus\n",
    "\n",
    "\n",
    "And then the problems started.\n",
    "\n",
    "- I haven't opened a PR yet, but the builder script for 'cornell_movie_dialog' is broken\n",
    "    \n",
    "    \\# y = load_dataset('cornell_movie_dialog')['train']  \n",
    "- 'Hello-SimpleAI/HC3' is broken too\n",
    "    \n",
    "    \\# z = load_dataset('Hello-SimpleAI/HC3')['train'] # ['question', 'human_answers']\n",
    "- The NPS Chat Corpus is a part of NLTK, and it's completely unusable because it is a loose collection of posts and not (somewhat coherent) dialogs. \n",
    "\n",
    "\n",
    "I was left with DailyDialogue and manually-processed HC3.\n",
    "\n",
    "By the way, these should've worked better if the data also contained prompts (like 'USER1 says '), in a manner in which the chatbot is prompted (see src/chatbot.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "\n",
    "dd = load_dataset('daily_dialog') # ['dialog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by the way, daily_dialogs is suddenly pre-tokenized in some places\n",
    "from sacremoses import MosesDetokenizer\n",
    "detok = MosesDetokenizer('en')\n",
    "\n",
    "def process_daily_dialogs(examples):\n",
    "    temp = [[detok.detokenize(x.split()) for x in y] for y in examples['dialog']]\n",
    "    temp = [' | '.join(x) for x in temp]\n",
    "    examples = tokenizer(temp)\n",
    "    return examples\n",
    "\n",
    "dd['train'] = dd['train'].map(process_daily_dialogs, batched=True, remove_columns=[\"dialog\", 'act', 'emotion'])\n",
    "dd['test'] = dd['test'].map(process_daily_dialogs, batched=True, remove_columns=[\"dialog\", 'act', 'emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/Hello-SimpleAI/HC3/resolve/main/all.jsonl\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_hc3():\n",
    "    with open(\"all.jsonl\") as inp:\n",
    "        data = pd.DataFrame.from_records([json.loads(line) for line in inp])\n",
    "    return data\n",
    "\n",
    "hc3 = Dataset.from_pandas(load_hc3()) # 'question', 'human_answers'\n",
    "hc3 = hc3.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hc3(examples):\n",
    "    # This could have been done in pandas, I think...\n",
    "    pairs = []\n",
    "    for x, y in zip(examples['question'], examples['human_answers']):\n",
    "        x = [detok.detokenize(x.split())] * len(y)\n",
    "        y = [detok.detokenize(t.split()) for t in y]\n",
    "        pairs.extend([' | '.join(d) for d in zip(x, y)])\n",
    "    examples = tokenizer(pairs)\n",
    "    return examples\n",
    "    \n",
    "hc3['train'] = hc3['train'].map(process_hc3, batched=True, remove_columns=[\"question\", 'human_answers', 'chatgpt_answers', 'index', 'source'])\n",
    "hc3['test'] = hc3['test'].map(process_hc3, batched=True, remove_columns=[\"question\", 'human_answers', 'chatgpt_answers', 'index', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "data_train = concatenate_datasets([dd['train'], hc3['train']]) \n",
    "data_test = concatenate_datasets([dd['test'], hc3['test']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "I'll use one PEFT-LoRa example, because I don't really have a lot of time to look deeper than that. I suppose that Prefix Tuning could work better, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a523349d9fae4bcdafabf6de7a766a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734543fbb7aa45849c77dc5a1e847194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model\n",
    "\n",
    "# I've seen people use int4 in BitsAndBytes for training; a bit too extreme for a baseline\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map={\"\":0})\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, lora_alpha=32, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19addb75d77c4665a007a974eb271fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS does not support cumsum op with int64 input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m      4\u001b[0m loader \u001b[39m=\u001b[39m DataCollatorForLanguageModeling(tokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      7\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      8\u001b[0m     train_dataset\u001b[39m=\u001b[39mdata_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     data_collator\u001b[39m=\u001b[39mloader\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/transformers/trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/transformers/trainer.py:2654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2653\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2654\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2657\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/transformers/trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2678\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2679\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2680\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/peft/peft_model.py:922\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    912\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    913\u001b[0m             input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    914\u001b[0m             attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    920\u001b[0m         )\n\u001b[0;32m--> 922\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m    923\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    924\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    925\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    926\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    927\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    928\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    929\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    930\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    933\u001b[0m batch_size \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/transformers/models/opt/modeling_opt.py:944\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    941\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    943\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m    945\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    946\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    947\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    948\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    949\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    950\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    951\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    952\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    953\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    954\u001b[0m )\n\u001b[1;32m    956\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    958\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/transformers/models/opt/modeling_opt.py:653\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    647\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe provided attention mask has length \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but its length should be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmask_seq_length\u001b[39m}\u001b[39;00m\u001b[39m (sum of the lengths of current and past inputs)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m     )\n\u001b[1;32m    650\u001b[0m causal_attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_decoder_attention_mask(\n\u001b[1;32m    651\u001b[0m     attention_mask, input_shape, inputs_embeds, past_key_values_length\n\u001b[1;32m    652\u001b[0m )\n\u001b[0;32m--> 653\u001b[0m pos_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_positions(attention_mask, past_key_values_length)\n\u001b[1;32m    655\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject_in \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject_in(inputs_embeds)\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/nlp_course/.venv/lib/python3.9/site-packages/transformers/models/opt/modeling_opt.py:114\u001b[0m, in \u001b[0;36mOPTLearnedPositionalEmbedding.forward\u001b[0;34m(self, attention_mask, past_key_values_length)\u001b[0m\n\u001b[1;32m    111\u001b[0m attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39mlong()\n\u001b[1;32m    113\u001b[0m \u001b[39m# create positions depending on attention_mask\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m positions \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39;49mcumsum(attention_mask, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mtype_as(attention_mask) \u001b[39m*\u001b[39m attention_mask)\u001b[39m.\u001b[39mlong() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[39m# cut positions if `past_key_values_length` is > 0\u001b[39;00m\n\u001b[1;32m    117\u001b[0m positions \u001b[39m=\u001b[39m positions[:, past_key_values_length:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS does not support cumsum op with int64 input"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "loader = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_test,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        # warmup_steps=500,\n",
    "        # max_steps=10000,\n",
    "        warmup_steps=1,\n",
    "        max_steps=1,\n",
    "        learning_rate=2e-4,  # ?\n",
    "        # fp16=True,\n",
    "        logging_steps=500,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=loader\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
